#!/usr/bin/env python3
"""
Script de traitement et d'indexation sÃ©mantique de documents Markdown
utilisant LlamaIndex avec des embeddings locaux (gratuit et sans limites).
ModÃ¨le: distiluse-base-multilingual-cased (optimisÃ© pour le franÃ§ais)
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from dotenv import load_dotenv

# LlamaIndex imports
from llama_index.core import (
    Document, 
    VectorStoreIndex, 
    Settings,
    StorageContext,
    load_index_from_storage
)
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.core.schema import NodeWithScore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.llms.ollama import Ollama

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()

class MarkdownDocumentProcessorLocal:
    """
    Processeur de documents Markdown avec embeddings locaux (gratuit).
    Utilise le modÃ¨le distiluse-base-multilingual-cased optimisÃ© pour le franÃ§ais.
    """
    
    def __init__(self, 
                 markdown_file_path: str,
                 storage_dir: str = "./storage",
                 embedding_model: str = "sentence-transformers/distiluse-base-multilingual-cased",
                 llm_model: str = "mistral:7b-instruct",
                 chunk_size: int = 1024,
                 chunk_overlap: int = 200):
        """
        Initialise le processeur de documents.
        
        Args:
            markdown_file_path: Chemin vers le fichier Markdown
            storage_dir: RÃ©pertoire pour stocker l'index
            embedding_model: ModÃ¨le d'embedding Ã  utiliser
            chunk_size: Taille des chunks
            chunk_overlap: Chevauchement entre chunks
        """
        self.markdown_file_path = Path(markdown_file_path)
        self.storage_dir = Path(storage_dir)
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # CrÃ©er le rÃ©pertoire de stockage s'il n'existe pas
        self.storage_dir.mkdir(exist_ok=True)
        
        # Configuration LlamaIndex avec embeddings locaux
        self._setup_llamaindex()
        
        # Variables pour l'index
        self.index = None
        self.query_engine = None
        
    def _setup_llamaindex(self):
        """Configure LlamaIndex avec des embeddings locaux."""
        logger.info(f"Configuration des embeddings locaux: {self.embedding_model}")
        
        # VÃ©rifier la disponibilitÃ© du GPU
        gpu_available = self._check_gpu()
        device = "cuda" if gpu_available else "cpu"
        logger.info(f"Device utilisÃ©: {device}")
        
        # Configuration des embeddings locaux
        Settings.embed_model = HuggingFaceEmbedding(
            model_name=self.embedding_model,
            device=device,
            # Optimisations pour la performance
            model_kwargs={
                "torch_dtype": "float16" if gpu_available else "float32",
                "trust_remote_code": True
            },
            # Cache des embeddings pour Ã©viter les recalculs
            cache_folder=str(self.storage_dir / "embeddings_cache")
        )
        
        # Configuration du LLM local (Ollama)
        try:
            Settings.llm = Ollama(
                model=self.llm_model,
                temperature=0.1,
                request_timeout=60.0
            )
            logger.info(f"LLM local configurÃ©: {self.llm_model}")
        except Exception as e:
            logger.warning(f"Impossible de configurer le LLM local: {e}")
            logger.info("Le LLM local sera configurÃ© plus tard si nÃ©cessaire")
        
        logger.info("Configuration LlamaIndex avec embeddings locaux terminÃ©e")
    
    def _check_gpu(self):
        """VÃ©rifie si un GPU est disponible."""
        try:
            import torch
            if torch.cuda.is_available():
                logger.info(f"GPU dÃ©tectÃ©: {torch.cuda.get_device_name(0)}")
                logger.info(f"VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
                return True
            else:
                logger.info("Aucun GPU dÃ©tectÃ©, utilisation du CPU")
                return False
        except ImportError:
            logger.warning("PyTorch non installÃ©, utilisation du CPU")
            return False
    
    def load_and_parse_markdown(self) -> List[Document]:
        """
        Charge et parse le fichier Markdown en prÃ©servant la structure hiÃ©rarchique.
        
        Returns:
            Liste de documents structurÃ©s
        """
        logger.info(f"Chargement du fichier: {self.markdown_file_path}")
        
        if not self.markdown_file_path.exists():
            raise FileNotFoundError(f"Fichier non trouvÃ©: {self.markdown_file_path}")
        
        # Lire le contenu du fichier
        with open(self.markdown_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Parser le Markdown en prÃ©servant la hiÃ©rarchie
        documents = self._parse_markdown_hierarchy(content)
        
        logger.info(f"Document chargÃ© et parsÃ©: {len(documents)} sections trouvÃ©es")
        return documents
    
    def _parse_markdown_hierarchy(self, content: str) -> List[Document]:
        """
        Parse le contenu Markdown en prÃ©servant la hiÃ©rarchie des titres.
        
        Args:
            content: Contenu du fichier Markdown
            
        Returns:
            Liste de documents avec mÃ©tadonnÃ©es de hiÃ©rarchie
        """
        documents = []
        lines = content.split('\n')
        current_section = {}
        current_content = []
        
        for i, line in enumerate(lines):
            # DÃ©tecter les titres (##, ###, etc.)
            if line.startswith('#'):
                # Sauvegarder la section prÃ©cÃ©dente si elle existe
                if current_section and current_content:
                    doc = self._create_document_from_section(current_section, current_content)
                    if doc:
                        documents.append(doc)
                
                # Commencer une nouvelle section
                level = len(line) - len(line.lstrip('#'))
                title = line.lstrip('#').strip()
                
                current_section = {
                    'title': title,
                    'level': level,
                    'line_number': i + 1
                }
                current_content = [line]
                
            else:
                # Ajouter le contenu Ã  la section courante
                if current_section:
                    current_content.append(line)
        
        # Traiter la derniÃ¨re section
        if current_section and current_content:
            doc = self._create_document_from_section(current_section, current_content)
            if doc:
                documents.append(doc)
        
        return documents
    
    def _create_document_from_section(self, section: Dict, content: List[str]) -> Optional[Document]:
        """
        CrÃ©e un document LlamaIndex Ã  partir d'une section.
        
        Args:
            section: MÃ©tadonnÃ©es de la section
            content: Contenu de la section
            
        Returns:
            Document LlamaIndex ou None si la section est vide
        """
        full_content = '\n'.join(content).strip()
        
        if not full_content or len(full_content) < 10:  # Ignorer les sections trop courtes
            return None
        
        # CrÃ©er le document avec mÃ©tadonnÃ©es
        doc = Document(
            text=full_content,
            metadata={
                'title': section['title'],
                'level': section['level'],
                'line_number': section['line_number'],
                'section_type': 'hierarchy',
                'source': str(self.markdown_file_path)
            }
        )
        
        return doc
    
    def create_chunks_with_hierarchy(self, documents: List[Document]) -> List[Document]:
        """
        CrÃ©e des chunks en prÃ©servant la hiÃ©rarchie des documents.
        
        Args:
            documents: Liste de documents Ã  dÃ©couper
            
        Returns:
            Liste de chunks avec mÃ©tadonnÃ©es de hiÃ©rarchie
        """
        logger.info("CrÃ©ation des chunks avec prÃ©servation de la hiÃ©rarchie...")
        
        # Utiliser le MarkdownNodeParser pour un dÃ©coupage intelligent
        node_parser = MarkdownNodeParser()
        
        all_chunks = []
        
        for doc in documents:
            try:
                # DÃ©couper chaque document en chunks
                nodes = node_parser.get_nodes_from_documents([doc])
                
                for node in nodes:
                    # Enrichir les mÃ©tadonnÃ©es du chunk
                    node.metadata.update({
                        'parent_title': doc.metadata.get('title', ''),
                        'parent_level': doc.metadata.get('level', 0),
                        'chunk_type': 'markdown_section'
                    })
                    
                    all_chunks.append(node)
                
            except Exception as e:
                logger.warning(f"Erreur lors du traitement du document {doc.metadata.get('title', 'Sans titre')}: {e}")
                continue
        
        logger.info(f"CrÃ©ation terminÃ©e: {len(all_chunks)} chunks gÃ©nÃ©rÃ©s")
        return all_chunks
    
    def build_index(self, force_rebuild: bool = False):
        """
        Construit l'index vectoriel pour la recherche sÃ©mantique.
        
        Args:
            force_rebuild: Force la reconstruction de l'index
        """
        index_path = self.storage_dir / "index"
        
        # VÃ©rifier si l'index existe dÃ©jÃ 
        if index_path.exists() and not force_rebuild:
            logger.info("Chargement de l'index existant...")
            try:
                storage_context = StorageContext.from_defaults(persist_dir=str(index_path))
                self.index = load_index_from_storage(storage_context)
                logger.info("Index chargÃ© avec succÃ¨s")
                return
            except Exception as e:
                logger.warning(f"Erreur lors du chargement de l'index: {e}")
                logger.info("Reconstruction de l'index...")
        
        # Construire un nouvel index
        logger.info("Construction d'un nouvel index avec embeddings locaux...")
        logger.info("âš ï¸  PremiÃ¨re exÃ©cution: tÃ©lÃ©chargement du modÃ¨le (peut prendre quelques minutes)")
        
        # Charger et parser les documents
        documents = self.load_and_parse_markdown()
        
        # CrÃ©er les chunks avec hiÃ©rarchie
        chunks = self.create_chunks_with_hierarchy(documents)
        
        # CrÃ©er l'index
        logger.info("GÃ©nÃ©ration des embeddings (cela peut prendre du temps)...")
        self.index = VectorStoreIndex(chunks)
        
        # Sauvegarder l'index
        self.index.storage_context.persist(persist_dir=str(index_path))
        logger.info(f"Index sauvegardÃ© dans: {index_path}")
        
        # CrÃ©er le moteur de requÃªte
        self._setup_query_engine()
    
    def _setup_query_engine(self):
        """Configure le moteur de requÃªte pour la recherche sÃ©mantique."""
        if self.index is None:
            raise ValueError("L'index doit Ãªtre construit avant de configurer le moteur de requÃªte")
        
        # Configurer le retriever
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=5
        )
        
        # CrÃ©er le moteur de requÃªte (sans LLM)
        self.query_engine = retriever
        
        logger.info("Moteur de requÃªte configurÃ© (recherche pure)")
    
    def search(self, query: str, top_k: int = 5) -> List[NodeWithScore]:
        """
        Effectue une recherche sÃ©mantique dans l'index.
        
        Args:
            query: RequÃªte de recherche
            top_k: Nombre de rÃ©sultats Ã  retourner
            
        Returns:
            Liste des nÅ“uds correspondants avec scores
        """
        if self.query_engine is None:
            raise ValueError("Le moteur de requÃªte n'est pas configurÃ©")
        
        logger.info(f"Recherche: '{query}'")
        
        # Effectuer la recherche
        nodes = self.query_engine.retrieve(query)
        
        return nodes
    
    def get_detailed_results(self, query: str) -> Dict[str, Any]:
        """
        Obtient des rÃ©sultats dÃ©taillÃ©s de recherche avec mÃ©tadonnÃ©es.
        
        Args:
            query: RequÃªte de recherche
            
        Returns:
            Dictionnaire avec les rÃ©sultats dÃ©taillÃ©s
        """
        results = self.search(query)
        
        detailed_results = {
            'query': query,
            'total_results': len(results),
            'results': []
        }
        
        for i, node in enumerate(results):
            result = {
                'rank': i + 1,
                'score': getattr(node, 'score', 0.0),
                'content': node.text[:200] + "..." if len(node.text) > 200 else node.text,
                'metadata': {
                    'title': node.metadata.get('title', 'Sans titre'),
                    'parent_title': node.metadata.get('parent_title', ''),
                    'level': node.metadata.get('level', 0),
                    'source': node.metadata.get('source', '')
                }
            }
            detailed_results['results'].append(result)
        
        return detailed_results
    
    def print_search_results(self, query: str):
        """
        Affiche les rÃ©sultats de recherche de maniÃ¨re formatÃ©e.
        
        Args:
            query: RequÃªte de recherche
        """
        results = self.get_detailed_results(query)
        
        print(f"\nğŸ” RÃ©sultats pour: '{query}'")
        print(f"ğŸ“Š {results['total_results']} rÃ©sultats trouvÃ©s\n")
        
        for result in results['results']:
            print(f"ğŸ“„ RÃ©sultat #{result['rank']} (Score: {result['score']:.3f})")
            print(f"   ğŸ“‘ Section: {result['metadata']['title']}")
            if result['metadata']['parent_title']:
                print(f"   ğŸ—ï¸  Parent: {result['metadata']['parent_title']}")
            print(f"   ğŸ“ Contenu: {result['content']}")
            print("-" * 80)
    
    def generate_qa_pairs(self, chunks: List[Document], output_file: str = "qa_pairs.jsonl"):
        """
        GÃ©nÃ¨re automatiquement des paires question/rÃ©ponse pour chaque chunk.
        
        Args:
            chunks: Liste des chunks de documents
            output_file: Fichier de sortie JSONL
        """
        logger.info(f"GÃ©nÃ©ration de paires Q/R pour {len(chunks)} chunks...")
        
        # VÃ©rifier que le LLM est configurÃ©
        if not hasattr(Settings, 'llm') or Settings.llm is None:
            logger.error("LLM non configurÃ©. Impossible de gÃ©nÃ©rer les paires Q/R.")
            return
        
        qa_pairs = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"Traitement du chunk {i+1}/{len(chunks)}")
            
            try:
                # GÃ©nÃ©rer des questions pour ce chunk
                questions = self._generate_questions_for_chunk(chunk)
                
                # Pour chaque question, gÃ©nÃ©rer une rÃ©ponse
                for question in questions:
                    answer = self._generate_answer_for_question(question, chunk)
                    
                    if question and answer:
                        qa_pair = {
                            "messages": [
                                {"role": "user", "content": question},
                                {"role": "assistant", "content": answer}
                            ]
                        }
                        qa_pairs.append(qa_pair)
                        logger.info(f"âœ… Paire Q/R gÃ©nÃ©rÃ©e: {question[:50]}...")
                
            except Exception as e:
                logger.warning(f"Erreur lors du traitement du chunk {i+1}: {e}")
                continue
        
        # Sauvegarder dans le fichier JSONL
        self._save_qa_pairs_to_jsonl(qa_pairs, output_file)
        
        logger.info(f"âœ… GÃ©nÃ©ration terminÃ©e: {len(qa_pairs)} paires Q/R sauvegardÃ©es dans {output_file}")
        return qa_pairs
    
    def _generate_questions_for_chunk(self, chunk: Document) -> List[str]:
        """
        GÃ©nÃ¨re des questions pour un chunk donnÃ©.
        
        Args:
            chunk: Chunk de document
            
        Returns:
            Liste de questions gÃ©nÃ©rÃ©es
        """
        prompt = f"""
BasÃ© sur le texte suivant, gÃ©nÃ¨re 2-3 questions pertinentes qui pourraient Ãªtre posÃ©es par un Ã©tudiant en droit administratif.

Texte:
{chunk.text}

GÃ©nÃ¨re des questions qui:
1. Testent la comprÃ©hension des concepts clÃ©s
2. Demandent des dÃ©finitions importantes
3. Explorent les diffÃ©rences entre concepts
4. Sont spÃ©cifiques au contenu du texte

Format: Une question par ligne, sans numÃ©rotation.
"""
        
        try:
            response = Settings.llm.complete(prompt)
            questions_text = str(response)
            
            # Parser les questions (une par ligne)
            questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
            
            # Filtrer les questions trop courtes ou vides
            questions = [q for q in questions if len(q) > 10 and '?' in q]
            
            return questions[:3]  # Limiter Ã  3 questions max
            
        except Exception as e:
            logger.warning(f"Erreur lors de la gÃ©nÃ©ration de questions: {e}")
            return []
    
    def _generate_answer_for_question(self, question: str, chunk: Document) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse pour une question donnÃ©e basÃ©e sur le chunk.
        
        Args:
            question: Question posÃ©e
            chunk: Chunk contenant les informations
            
        Returns:
            RÃ©ponse gÃ©nÃ©rÃ©e
        """
        prompt = f"""
Tu es un assistant spÃ©cialisÃ© en droit administratif. RÃ©ponds Ã  la question suivante en te basant uniquement sur le texte fourni.

Question: {question}

Texte de rÃ©fÃ©rence:
{chunk.text}

Instructions:
1. RÃ©ponds uniquement en franÃ§ais
2. Utilise uniquement les informations du texte fourni
3. Sois prÃ©cis et concis
4. Si la rÃ©ponse n'est pas dans le texte, dis-le clairement
5. Structure ta rÃ©ponse de maniÃ¨re claire

RÃ©ponse:
"""
        
        try:
            response = Settings.llm.complete(prompt)
            return str(response).strip()
            
        except Exception as e:
            logger.warning(f"Erreur lors de la gÃ©nÃ©ration de rÃ©ponse: {e}")
            return ""
    
    def _save_qa_pairs_to_jsonl(self, qa_pairs: List[Dict], output_file: str):
        """
        Sauvegarde les paires Q/R dans un fichier JSONL.
        
        Args:
            qa_pairs: Liste des paires Q/R
            output_file: Nom du fichier de sortie
        """
        output_path = self.storage_dir / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for qa_pair in qa_pairs:
                json.dump(qa_pair, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"âœ… {len(qa_pairs)} paires Q/R sauvegardÃ©es dans {output_path}")
    
    def generate_training_data(self, output_file: str = "training_data.jsonl"):
        """
        GÃ©nÃ¨re un dataset d'entraÃ®nement complet Ã  partir du document.
        
        Args:
            output_file: Fichier de sortie pour le dataset
        """
        logger.info("ğŸš€ GÃ©nÃ©ration du dataset d'entraÃ®nement...")
        
        # Charger et parser les documents
        documents = self.load_and_parse_markdown()
        
        # CrÃ©er les chunks
        chunks = self.create_chunks_with_hierarchy(documents)
        
        # GÃ©nÃ©rer les paires Q/R
        qa_pairs = self.generate_qa_pairs(chunks, output_file)
        
        # Statistiques
        logger.info(f"ğŸ“Š Dataset gÃ©nÃ©rÃ©:")
        logger.info(f"   ğŸ“„ Documents: {len(documents)}")
        logger.info(f"   ğŸ“ Chunks: {len(chunks)}")
        logger.info(f"   â“ Paires Q/R: {len(qa_pairs)}")
        logger.info(f"   ğŸ’¾ Fichier: {self.storage_dir / output_file}")
        
        return qa_pairs
    
    def get_model_info(self):
        """Affiche les informations sur le modÃ¨le utilisÃ©."""
        print("\nğŸ¤– INFORMATIONS DU MODÃˆLE")
        print("=" * 50)
        print(f"ğŸ“¦ ModÃ¨le: {self.embedding_model}")
        print(f"ğŸŒ Multilingue: Oui (optimisÃ© pour le franÃ§ais)")
        print(f"ğŸ’¾ Taille: ~471MB")
        print(f"ğŸ§  Dimension des embeddings: 512")
        print(f"âš¡ Device: {'GPU' if self._check_gpu() else 'CPU'}")
        print(f"ğŸ’° CoÃ»t: Gratuit (local)")


def main():
    """
    Fonction principale pour dÃ©montrer l'utilisation du processeur local.
    """
    # Configuration
    markdown_file = "/Users/nicolas/Documents/Travail/huggingface/txt-to-chunks/local-embedder/sources/droitadminSmall.md"
    storage_dir = "/Users/nicolas/Documents/Travail/huggingface/txt-to-chunks/local-embedder/storage"
    
    try:
        # Initialiser le processeur
        processor = MarkdownDocumentProcessorLocal(
            markdown_file_path=markdown_file,
            storage_dir=storage_dir,
            embedding_model="sentence-transformers/distiluse-base-multilingual-cased"
        )
        
        print("ğŸš€ Initialisation du processeur de documents (version locale)...")
        
        # Afficher les informations du modÃ¨le
        processor.get_model_info()
        
        # Construire l'index
        processor.build_index(force_rebuild=False)
        
        print("âœ… Index construit avec succÃ¨s (embeddings locaux)!")
        
        # Menu principal
        while True:
            print("\n" + "="*80)
            print("ğŸ¯ MENU PRINCIPAL")
            print("="*80)
            print("1. ğŸ” Recherche sÃ©mantique")
            print("2. ğŸ¤– GÃ©nÃ©rer dataset d'entraÃ®nement (Q/R)")
            print("3. ğŸ“Š Afficher statistiques")
            print("4. âŒ Quitter")
            
            choice = input("\nChoisissez une option (1-4): ").strip()
            
            if choice == "1":
                # Recherche sÃ©mantique
                print("\n" + "="*80)
                print("ğŸ” RECHERCHE SÃ‰MANTIQUE")
                print("="*80)
                
                # Exemples de recherche
                example_queries = [
                    "Qu'est-ce que l'administration?",
                    "DÃ©finition des personnes publiques",
                    "DiffÃ©rence entre Ã©tablissement public et collectivitÃ© territoriale",
                    "Service public et externalisation",
                    "Organes des personnes publiques"
                ]
                
                print("Exemples de requÃªtes:")
                for i, query in enumerate(example_queries, 1):
                    print(f"  {i}. {query}")
                
                print("\nTapez vos questions (ou 'back' pour retourner au menu):")
                
                while True:
                    user_query = input("\nâ“ Votre question: ").strip()
                    
                    if user_query.lower() in ['back', 'retour']:
                        break
                    elif user_query:
                        processor.print_search_results(user_query)
            
            elif choice == "2":
                # GÃ©nÃ©ration de dataset d'entraÃ®nement
                print("\n" + "="*80)
                print("ğŸ¤– GÃ‰NÃ‰RATION DE DATASET D'ENTRAÃNEMENT")
                print("="*80)
                print("âš ï¸  Cette opÃ©ration peut prendre du temps (5-15 minutes)")
                print("ğŸ“ GÃ©nÃ©ration de paires question/rÃ©ponse pour chaque passage...")
                
                confirm = input("\nContinuer? (y/N): ").strip().lower()
                if confirm in ['y', 'yes', 'oui']:
                    try:
                        qa_pairs = processor.generate_training_data("training_data.jsonl")
                        print(f"\nâœ… Dataset gÃ©nÃ©rÃ© avec succÃ¨s!")
                        print(f"ğŸ“Š {len(qa_pairs)} paires Q/R crÃ©Ã©es")
                        print(f"ğŸ’¾ Fichier: {processor.storage_dir / 'training_data.jsonl'}")
                    except Exception as e:
                        print(f"âŒ Erreur lors de la gÃ©nÃ©ration: {e}")
                else:
                    print("âŒ GÃ©nÃ©ration annulÃ©e")
            
            elif choice == "3":
                # Statistiques
                print("\n" + "="*80)
                print("ğŸ“Š STATISTIQUES")
                print("="*80)
                
                documents = processor.load_and_parse_markdown()
                chunks = processor.create_chunks_with_hierarchy(documents)
                
                print(f"ğŸ“„ Documents parsÃ©s: {len(documents)}")
                print(f"ğŸ“ Chunks crÃ©Ã©s: {len(chunks)}")
                print(f"ğŸ’¾ RÃ©pertoire de stockage: {processor.storage_dir}")
                
                # VÃ©rifier si des donnÃ©es d'entraÃ®nement existent
                training_file = processor.storage_dir / "training_data.jsonl"
                if training_file.exists():
                    with open(training_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    print(f"ğŸ¤– Paires Q/R gÃ©nÃ©rÃ©es: {len(lines)}")
                else:
                    print("ğŸ¤– Aucune donnÃ©e d'entraÃ®nement gÃ©nÃ©rÃ©e")
            
            elif choice == "4":
                print("ğŸ‘‹ Au revoir!")
                break
            
            else:
                print("âŒ Option invalide, veuillez choisir 1-4")
    
    except Exception as e:
        logger.error(f"Erreur: {e}")
        print(f"âŒ Erreur: {e}")


if __name__ == "__main__":
    main()
