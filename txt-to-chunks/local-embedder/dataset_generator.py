#!/usr/bin/env python3
"""
G√©n√©rateur de dataset Q/R √† partir de documents Markdown.
Utilise des design patterns pour une architecture claire et maintenable.
"""

import os
import json
import logging
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional, Protocol
from dataclasses import dataclass
from dotenv import load_dotenv

# LlamaIndex imports
from llama_index.core import Document
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.llms.ollama import Ollama

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()

# Charger les variables d'environnement si disponibles
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


@dataclass
class QAPair:
    """Repr√©sente une paire question/r√©ponse."""
    question: str
    answer: str
    source_title: str
    source_level: int
    
    def to_jsonl_format(self) -> Dict[str, Any]:
        """Convertit en format JSONL standard."""
        return {
            "messages": [
                {"role": "user", "content": self.question},
                {"role": "assistant", "content": self.answer}
            ]
        }


class MarkdownParser(ABC):
    """Interface pour le parsing de documents Markdown."""
    
    @abstractmethod
    def parse(self, content: str) -> List[Document]:
        """Parse le contenu Markdown en documents."""
        pass


class HierarchicalMarkdownParser(MarkdownParser):
    """Parser Markdown qui pr√©serve la hi√©rarchie des titres."""
    
    def parse(self, content: str) -> List[Document]:
        """Parse le contenu en pr√©servant la structure hi√©rarchique."""
        documents = []
        lines = content.split('\n')
        current_section = {}
        current_content = []
        
        for i, line in enumerate(lines):
            if line.startswith('#'):
                # Sauvegarder la section pr√©c√©dente
                if current_section and current_content:
                    doc = self._create_document_from_section(current_section, current_content)
                    if doc:
                        documents.append(doc)
                
                # Nouvelle section
                level = len(line) - len(line.lstrip('#'))
                title = line.lstrip('#').strip()
                
                current_section = {
                    'title': title,
                    'level': level,
                    'line_number': i + 1
                }
                current_content = [line]
            else:
                if current_section:
                    current_content.append(line)
        
        # Derni√®re section
        if current_section and current_content:
            doc = self._create_document_from_section(current_section, current_content)
            if doc:
                documents.append(doc)
        
        return documents
    
    def _create_document_from_section(self, section: Dict, content: List[str]) -> Optional[Document]:
        """Cr√©e un document √† partir d'une section."""
        full_content = '\n'.join(content).strip()
        
        if not full_content or len(full_content) < 10:
            return None
        
        return Document(
            text=full_content,
            metadata={
                'title': section['title'],
                'level': section['level'],
                'line_number': section['line_number'],
                'section_type': 'hierarchy'
            }
        )


class ChunkProcessor(ABC):
    """Interface pour le traitement des chunks."""
    
    @abstractmethod
    def process(self, documents: List[Document]) -> List[Document]:
        """Traite les documents en chunks."""
        pass


class MarkdownChunkProcessor(ChunkProcessor):
    """Processeur de chunks sp√©cialis√© pour Markdown."""
    
    def __init__(self):
        self.node_parser = MarkdownNodeParser()
    
    def process(self, documents: List[Document]) -> List[Document]:
        """D√©coupe les documents en chunks s√©mantiques."""
        all_chunks = []
        
        for doc in documents:
            try:
                nodes = self.node_parser.get_nodes_from_documents([doc])
                
                for node in nodes:
                    # Enrichir les m√©tadonn√©es
                    node.metadata.update({
                        'parent_title': doc.metadata.get('title', ''),
                        'parent_level': doc.metadata.get('level', 0),
                        'chunk_type': 'markdown_section'
                    })
                    all_chunks.append(node)
            except Exception as e:
                logger.warning(f"Erreur lors du traitement du document {doc.metadata.get('title', 'Sans titre')}: {e}")
                continue
        
        return all_chunks


class QuestionGenerator(ABC):
    """Interface pour la g√©n√©ration de questions."""
    
    @abstractmethod
    def generate_questions(self, chunk: Document) -> List[str]:
        """G√©n√®re des questions pour un chunk."""
        pass


class LLMQuestionGenerator(QuestionGenerator):
    """G√©n√©rateur de questions utilisant un LLM."""
    
    def __init__(self, llm_model: str = "mistral:7b-instruct"):
        self.llm = Ollama(
            model=llm_model,
            temperature=0.1,
            request_timeout=60.0
        )
    
    def generate_questions(self, chunk: Document) -> List[str]:
        """G√©n√®re des questions pertinentes pour un chunk."""
        prompt = f"""
Bas√© sur le texte suivant, g√©n√®re 2-3 questions en fran√ßais pertinentes qui pourraient √™tre pos√©es par un √©tudiant en droit administratif.

Texte:
{chunk.text}

G√©n√®re des questions uniquement en fran√ßais qui:
1. Testent la compr√©hension des concepts cl√©s
2. Demandent des d√©finitions importantes
3. Explorent les diff√©rences entre concepts
4. Sont sp√©cifiques au contenu du texte

Format: Une question par ligne, sans num√©rotation.
"""
        
        try:
            response = self.llm.complete(prompt)
            questions_text = str(response)
            
            # Parser les questions
            questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
            questions = [q for q in questions if len(q) > 10 and '?' in q]
            
            return questions[:3]  # Limiter √† 3 questions max
            
        except Exception as e:
            logger.warning(f"Erreur lors de la g√©n√©ration de questions: {e}")
            return []


class AnswerGenerator(ABC):
    """Interface pour la g√©n√©ration de r√©ponses."""
    
    @abstractmethod
    def generate_answer(self, question: str, chunk: Document) -> str:
        """G√©n√®re une r√©ponse pour une question."""
        pass


class LLMAnswerGenerator(AnswerGenerator):
    """G√©n√©rateur de r√©ponses utilisant un LLM."""
    
    def __init__(self, llm_model: str = "mistral:7b-instruct"):
        self.llm = Ollama(
            model=llm_model,
            temperature=0.1,
            request_timeout=60.0
        )
    
    def generate_answer(self, question: str, chunk: Document) -> str:
        """G√©n√®re une r√©ponse bas√©e sur le chunk."""
        prompt = f"""
Tu es un assistant sp√©cialis√© en droit administratif. R√©ponds en fran√ßais √† la question suivante en te basant uniquement sur le texte fourni.

Question: {question}

Texte de r√©f√©rence:
{chunk.text}

Instructions:
1. R√©ponds uniquement en fran√ßais
2. Utilise uniquement les informations du texte fourni
3. Sois pr√©cis et concis
4. Si la r√©ponse n'est pas dans le texte, dis-le clairement
5. Structure ta r√©ponse de mani√®re claire

R√©ponse:
"""
        
        try:
            response = self.llm.complete(prompt)
            return str(response).strip()
        except Exception as e:
            logger.warning(f"Erreur lors de la g√©n√©ration de r√©ponse: {e}")
            return ""


class QAPairGenerator:
    """G√©n√©rateur de paires Q/R utilisant le pattern Strategy."""
    
    def __init__(self, question_generator: QuestionGenerator, answer_generator: AnswerGenerator):
        self.question_generator = question_generator
        self.answer_generator = answer_generator
    
    def generate_qa_pairs(self, chunks: List[Document]) -> List[QAPair]:
        """G√©n√®re des paires Q/R pour tous les chunks."""
        qa_pairs = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"Traitement du chunk {i+1}/{len(chunks)}")
            
            try:
                # G√©n√©rer des questions
                questions = self.question_generator.generate_questions(chunk)
                
                # G√©n√©rer des r√©ponses
                for question in questions:
                    answer = self.answer_generator.generate_answer(question, chunk)
                    
                    if question and answer:
                        qa_pair = QAPair(
                            question=question,
                            answer=answer,
                            source_title=chunk.metadata.get('title', 'Sans titre'),
                            source_level=chunk.metadata.get('level', 0)
                        )
                        qa_pairs.append(qa_pair)
                        logger.info(f"‚úÖ Paire Q/R g√©n√©r√©e: {question[:50]}...")
                
            except Exception as e:
                logger.warning(f"Erreur lors du traitement du chunk {i+1}: {e}")
                continue
        
        return qa_pairs


class DatasetExporter(ABC):
    """Interface pour l'export de datasets."""
    
    @abstractmethod
    def export(self, qa_pairs: List[QAPair], output_path: Path) -> None:
        """Exporte les paires Q/R dans un fichier."""
        pass


class JSONLExporter(DatasetExporter):
    """Exporteur au format JSONL."""
    
    def export(self, qa_pairs: List[QAPair], output_path: Path) -> None:
        """Exporte en format JSONL standard."""
        with open(output_path, 'w', encoding='utf-8') as f:
            for qa_pair in qa_pairs:
                json.dump(qa_pair.to_jsonl_format(), f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"‚úÖ {len(qa_pairs)} paires Q/R export√©es vers {output_path}")


class DatasetGenerator:
    """G√©n√©rateur principal utilisant le pattern Facade."""
    
    def __init__(self, 
                 markdown_parser: MarkdownParser,
                 chunk_processor: ChunkProcessor,
                 qa_generator: QAPairGenerator,
                 exporter: DatasetExporter):
        self.markdown_parser = markdown_parser
        self.chunk_processor = chunk_processor
        self.qa_generator = qa_generator
        self.exporter = exporter
    
    def generate_dataset(self, 
                        markdown_file: Path, 
                        output_file: Path) -> List[QAPair]:
        """G√©n√®re un dataset complet √† partir d'un fichier Markdown."""
        logger.info(f"üöÄ G√©n√©ration du dataset depuis {markdown_file}")
        
        # 1. Charger et parser le Markdown
        with open(markdown_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        documents = self.markdown_parser.parse(content)
        logger.info(f"üìÑ {len(documents)} sections pars√©es")
        
        # 2. Traiter en chunks
        chunks = self.chunk_processor.process(documents)
        logger.info(f"üìù {len(chunks)} chunks cr√©√©s")
        
        # 3. G√©n√©rer les paires Q/R
        qa_pairs = self.qa_generator.generate_qa_pairs(chunks)
        logger.info(f"‚ùì {len(qa_pairs)} paires Q/R g√©n√©r√©es")
        
        # 4. Exporter
        self.exporter.export(qa_pairs, output_file)
        
        return qa_pairs


class DatasetGeneratorFactory:
    """Factory pour cr√©er des g√©n√©rateurs de dataset."""
    
    @staticmethod
    def create_default_generator(llm_model: str = "mistral:7b-instruct") -> DatasetGenerator:
        """Cr√©e un g√©n√©rateur avec la configuration par d√©faut."""
        # Composants
        markdown_parser = HierarchicalMarkdownParser()
        chunk_processor = MarkdownChunkProcessor()
        
        # LLM components
        question_generator = LLMQuestionGenerator(llm_model)
        answer_generator = LLMAnswerGenerator(llm_model)
        qa_generator = QAPairGenerator(question_generator, answer_generator)
        
        # Export
        exporter = JSONLExporter()
        
        return DatasetGenerator(
            markdown_parser=markdown_parser,
            chunk_processor=chunk_processor,
            qa_generator=qa_generator,
            exporter=exporter
        )


def main():
    """Fonction principale."""
    # Configuration
    markdown_file = Path("sources/droitadminSmall.md")
    output_file = Path("training_data.jsonl")
    
    try:
        # Cr√©er le g√©n√©rateur
        generator = DatasetGeneratorFactory.create_default_generator()
        
        # G√©n√©rer le dataset
        qa_pairs = generator.generate_dataset(markdown_file, output_file)
        
        print(f"\n‚úÖ Dataset g√©n√©r√© avec succ√®s!")
        print(f"üìä {len(qa_pairs)} paires Q/R cr√©√©es")
        print(f"üíæ Fichier: {output_file}")
        
    except Exception as e:
        logger.error(f"Erreur: {e}")
        print(f"‚ùå Erreur: {e}")


if __name__ == "__main__":
    main()
