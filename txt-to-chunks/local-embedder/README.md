# Local Embedder - Recherche SÃ©mantique et GÃ©nÃ©ration Q/R Locale

Ce projet utilise des **embeddings locaux** pour la recherche sÃ©mantique dans des documents Markdown, et gÃ©nÃ¨re automatiquement des **paires question/rÃ©ponse** pour l'entraÃ®nement de modÃ¨les, sans dÃ©pendre d'APIs externes coÃ»teuses.

## ğŸš€ **Avantages de la solution locale**

- âœ… **Gratuit** : Pas de coÃ»ts d'API
- âœ… **Sans limites** : Pas de rate limits
- âœ… **Rapide** : Pas de latence rÃ©seau
- âœ… **PrivÃ©** : DonnÃ©es restent locales
- âœ… **ContrÃ´le total** : Personnalisation possible
- âœ… **GÃ©nÃ©ration Q/R** : CrÃ©ation automatique de datasets d'entraÃ®nement

## ğŸ¤– **ModÃ¨les utilisÃ©s**

### **Embeddings**
**`sentence-transformers/distiluse-base-multilingual-cased`**
- ğŸŒ **Multilingue** : OptimisÃ© pour le franÃ§ais
- ğŸ’¾ **Taille** : ~471MB
- ğŸ§  **Dimension** : 512
- âš¡ **Performance** : Excellente qualitÃ©

### **LLM pour gÃ©nÃ©ration Q/R**
**`mistral:7b-instruct`**
- ğŸ§  **ModÃ¨le** : Mistral 7B Instruct
- ğŸ’¾ **Taille** : ~4.1GB
- ğŸŒ **Multilingue** : Excellent en franÃ§ais
- âš¡ **Performance** : TrÃ¨s bonne qualitÃ©

## ğŸ“‹ **PrÃ©requis**

1. **Python 3.8+**
2. **RAM** : 8GB minimum (16GB recommandÃ© pour le LLM)
3. **GPU** : Optionnel mais recommandÃ© pour la vitesse
4. **Espace disque** : 6GB pour les modÃ¨les et l'index
5. **Ollama** : Pour le LLM local (Mistral 7B)

## ğŸ› ï¸ **Installation**

### 1. **Installer les dÃ©pendances**

```bash
cd local-embedder
pip install -r requirements.txt
```

### 2. **Installation d'Ollama et du modÃ¨le Mistral**

#### **Sur macOS (recommandÃ©)**
```bash
# Installation automatique pour macOS
./install_macos.sh

# Ou installation manuelle
# Voir INSTALL_MACOS.md pour le guide dÃ©taillÃ©
```

#### **Sur Linux**
```bash
# Installation automatique
./install_ollama.sh

# Ou installation manuelle
# 1. TÃ©lÃ©chargez Ollama: https://ollama.ai
# 2. Installez le modÃ¨le: ollama pull mistral:7b-instruct
```

### 3. **Installation GPU (optionnel mais recommandÃ©)**

Pour utiliser le GPU et accÃ©lÃ©rer les calculs :

```bash
# Pour CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Pour CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## ğŸ¯ **Utilisation**

### **Test de la configuration**

```bash
# Test des embeddings
python test_local.py

# Test de la gÃ©nÃ©ration Q/R
python test_qa_generation.py
```

### **ExÃ©cution du script principal**

```bash
python main.py
```

Le script propose un menu interactif avec :
1. **ğŸ” Recherche sÃ©mantique** : Questions sur le document
2. **ğŸ¤– GÃ©nÃ©ration Q/R** : CrÃ©ation automatique de paires question/rÃ©ponse
3. **ğŸ“Š Statistiques** : Informations sur le dataset
4. **âŒ Quitter**

## ğŸ¤– **GÃ©nÃ©ration de donnÃ©es d'entraÃ®nement**

### **Format JSONL gÃ©nÃ©rÃ©**

Chaque ligne du fichier `training_data.jsonl` contient une paire question/rÃ©ponse :

```json
{"messages": [{"role": "user", "content": "Qu'est-ce que l'administration?"}, {"role": "assistant", "content": "L'administration est l'ensemble des personnes publiques franÃ§aises."}]}
```

### **Processus de gÃ©nÃ©ration**

1. **Parsing** : DÃ©coupage du document en chunks sÃ©mantiques
2. **GÃ©nÃ©ration de questions** : 2-3 questions par chunk via Mistral 7B
3. **GÃ©nÃ©ration de rÃ©ponses** : RÃ©ponses basÃ©es sur le contenu du chunk
4. **Formatage** : Conversion au format JSONL standard

### **Exemple d'utilisation**

```bash
# GÃ©nÃ©rer le dataset complet
python main.py
# Choisir l'option 2

# Le fichier sera crÃ©Ã© dans: storage/training_data.jsonl
```

## ğŸ”§ **Configuration**

### **ModÃ¨les d'embedding disponibles**

| ModÃ¨le | Taille | RAM | Performance | FranÃ§ais |
|--------|--------|-----|-------------|----------|
| `all-MiniLM-L6-v2` | 22MB | 1GB | â­â­â­ | â­â­â­ |
| `paraphrase-multilingual` | 118MB | 2GB | â­â­â­â­ | â­â­â­â­ |
| **`distiluse-base-multilingual`** | **471MB** | **4GB** | **â­â­â­â­â­** | **â­â­â­â­â­** |

### **Configuration personnalisÃ©e**

```python
processor = MarkdownDocumentProcessorLocal(
    markdown_file_path="chemin/vers/fichier.md",
    storage_dir="./storage",
    embedding_model="sentence-transformers/distiluse-base-multilingual-cased",
    chunk_size=1024,
    chunk_overlap=200
)
```

## ğŸ“Š **Performance**

### **PremiÃ¨re exÃ©cution**
- **TÃ©lÃ©chargement du modÃ¨le** : 2-5 minutes
- **GÃ©nÃ©ration des embeddings** : 1-3 minutes
- **Total** : 3-8 minutes

### **ExÃ©cutions suivantes**
- **Chargement de l'index** : 5-10 secondes
- **Recherche** : 0.1-0.5 secondes

## ğŸ–¥ï¸ **Support GPU**

### **VÃ©rification GPU**

```python
import torch
print(f"GPU disponible: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"Device: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
```

### **Configuration GPU automatique**

Le script dÃ©tecte automatiquement la disponibilitÃ© du GPU et l'utilise si possible.

## ğŸ“ **Structure des fichiers**

```
local-embedder/
â”œâ”€â”€ main.py                 # Script principal
â”œâ”€â”€ test_local.py          # Script de test
â”œâ”€â”€ requirements.txt       # DÃ©pendances
â”œâ”€â”€ README.md             # Documentation
â””â”€â”€ storage/              # Index et cache (gÃ©nÃ©rÃ©)
    â”œâ”€â”€ index/            # Index vectoriel
    â””â”€â”€ embeddings_cache/ # Cache des embeddings
```

## ğŸ” **Exemples de recherche**

Le script inclut des exemples de recherche sÃ©mantique :

- "Qu'est-ce que l'administration?"
- "DÃ©finition des personnes publiques"
- "DiffÃ©rence entre Ã©tablissement public et collectivitÃ© territoriale"
- "Service public et externalisation"

## âš¡ **Optimisations**

### **Cache des embeddings**
Les embeddings sont mis en cache pour Ã©viter les recalculs.

### **Quantification (optionnel)**
```python
# Pour rÃ©duire l'utilisation mÃ©moire
model_kwargs={"torch_dtype": "float16"}
```

### **Batch processing**
Le script traite les documents par batch pour optimiser la mÃ©moire.

## ğŸš¨ **DÃ©pannage**

### **Erreur de mÃ©moire**
- RÃ©duisez `chunk_size` (512 au lieu de 1024)
- Utilisez un modÃ¨le plus petit
- Fermez d'autres applications

### **Erreur de tÃ©lÃ©chargement**
- VÃ©rifiez votre connexion internet
- Le modÃ¨le sera tÃ©lÃ©chargÃ© automatiquement

### **Performance lente**
- Installez PyTorch avec support GPU
- Utilisez un modÃ¨le plus petit si nÃ©cessaire

## ğŸ“ˆ **Comparaison avec les APIs**

| Solution | CoÃ»t | Vitesse | Limites | PrivÃ© |
|----------|------|---------|---------|-------|
| **Local** | Gratuit | Rapide | Aucune | âœ… |
| OpenAI | Payant | Rapide | Rate limits | âŒ |
| Mistral | Payant | Rapide | Rate limits | âŒ |

## ğŸ¯ **Recommandations**

1. **PremiÃ¨re utilisation** : Lancez `test_local.py` pour vÃ©rifier la configuration
2. **GPU recommandÃ©** : Pour des performances optimales
3. **ModÃ¨le distiluse** : Meilleur Ã©quilibre qualitÃ©/performance pour le franÃ§ais
4. **Cache activÃ©** : Les embeddings sont mis en cache automatiquement

## ğŸ”§ **Personnalisation avancÃ©e**

### **ModÃ¨le personnalisÃ©**
```python
# Utiliser un modÃ¨le diffÃ©rent
processor = MarkdownDocumentProcessorLocal(
    embedding_model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
```

### **Configuration GPU manuelle**
```python
# Forcer l'utilisation du CPU
Settings.embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/distiluse-base-multilingual-cased",
    device="cpu"
)
```

Cette solution locale vous donne un contrÃ´le total sur vos donnÃ©es et vos coÃ»ts !
